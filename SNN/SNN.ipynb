{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import struct\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import snntorch as snn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import snntorch.spikeplot as splt\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchmetrics.classification import MulticlassConfusionMatrix\n",
    "\n",
    "from snntorch import surrogate\n",
    "import snntorch.functional as SF\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import NLLLoss, LogSoftmax\n",
    "\n",
    "from snntorch import spikegen\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import SymLogNorm\n",
    "import snntorch.spikeplot as splt\n",
    "import imageio\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import visualize\n",
    "import dataset as ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nCublets = 1000\n",
    "nSensors = 100\n",
    "max_t = 20\n",
    "dt = 0.2\n",
    "timesteps = int(max_t/dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = {\n",
    "  -1: \"unclassified\",\n",
    "   0: \"proton\",\n",
    "   1: \"kaon\",\n",
    "   2: \"pion\",\n",
    "   3: \"other\"\n",
    "}\n",
    "nClasses = len(labels_map)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNN Definition and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Spiking_Net(nn.Module):\n",
    "    \"\"\"FCN with variable neural model and number of layers.\"\"\"\n",
    "\n",
    "    def __init__(self, net_desc, spikegen_fn):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_neurons = net_desc[\"layers\"]\n",
    "        self.timesteps = net_desc[\"timesteps\"]\n",
    "        self.output = net_desc[\"output\"]\n",
    "\n",
    "        modules = []\n",
    "        for i_layer in range(1, len(self.n_neurons)):\n",
    "            modules.append(nn.Linear(in_features=self.n_neurons[i_layer-1], out_features=self.n_neurons[i_layer]))\n",
    "            if \"model\" in net_desc:\n",
    "                modules.append(net_desc[\"model\"](**net_desc[\"neuron_params\"][i_layer]))\n",
    "            else:\n",
    "                modules.append(net_desc[\"neuron_params\"][i_layer][0](**(net_desc[\"neuron_params\"][i_layer][1])))\n",
    "        self.network = nn.Sequential(*modules)\n",
    "\n",
    "        self.spikegen_fn = spikegen_fn\n",
    "\n",
    "    \n",
    "    def forward(self, data):\n",
    "        \"\"\"Forward pass for several time steps.\"\"\"\n",
    "\n",
    "        x = self.spikegen_fn(data)\n",
    "\n",
    "        # Initalize membrane potential\n",
    "        mem = []\n",
    "        for i, module in enumerate(self.network):\n",
    "            if i%2==1:\n",
    "                res = module.reset_mem()\n",
    "                if type(res) is tuple:\n",
    "                    mem.append(list(res))\n",
    "                else:\n",
    "                    mem.append([res])\n",
    "\n",
    "        # Record the final layer\n",
    "        spk_rec = []\n",
    "        mem_rec = []\n",
    "\n",
    "        # Loop over \n",
    "        spk = None\n",
    "        for step in range(self.timesteps):\n",
    "            for i_layer in range(len(self.network)//2):\n",
    "                if i_layer == 0:\n",
    "                    cur = self.network[2*i_layer](x[step])\n",
    "                else:\n",
    "                    cur = self.network[2*i_layer](spk)\n",
    "                \n",
    "                spk, *(mem[i_layer]) = self.network[2*i_layer+1](cur, *(mem[i_layer]))\n",
    "\n",
    "                if i_layer == len(self.network)//2-1:\n",
    "                    spk_rec.append(spk)\n",
    "                    mem_rec.append(mem[i_layer][-1])\n",
    "\n",
    "        if self.output == \"spike\":\n",
    "            return torch.stack(spk_rec, dim=0)\n",
    "        elif self.output == \"membrane\":\n",
    "            return torch.stack(mem_rec, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "\n",
    "    def __init__(self, net, loss_fn, optimizer,\n",
    "                 train_dataset, val_dataset, test_dataset, predict=None):\n",
    "        self.net = net\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.datasets = {\"train\": train_dataset, \"validation\": val_dataset, \"test\": test_dataset}\n",
    "        self.predict = predict\n",
    "\n",
    "        self.current_epoch = 0\n",
    "        self.loss_hist = {\"train\": {}, \"validation\": {}, \"test\": {}}\n",
    "        self.acc_hist = {\"validation\": {}, \"test\": {}}    \n",
    "\n",
    "\n",
    "    def test(self, dataset_name, transform=None):\n",
    "\n",
    "        # by default, computes accuracy on test dataset\n",
    "        try:\n",
    "            if dataset_name == \"validation\" or dataset_name == \"test\":\n",
    "                dataset = self.datasets[dataset_name]\n",
    "            else:\n",
    "                raise NameError(\"Unidentified dataset name. Please choose between \\\"validation\\\" or \\\"test\\\".\")\n",
    "        except NameError as n:\n",
    "            print(f\"Error: {n}\")\n",
    "\n",
    "        temp_loss = []\n",
    "        temp_acc = []\n",
    "\n",
    "        self.net.eval()\n",
    "        with torch.no_grad():\n",
    "            temp_loss = []\n",
    "            for data, targets in dataset:\n",
    "                data = data.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                # forward pass\n",
    "                output = self.net(data)\n",
    "                pred, _ = self.predict(output, targets)\n",
    "\n",
    "                # compute loss\n",
    "                loss = self.loss_fn(pred, targets)\n",
    "                temp_loss.append(loss.item())\n",
    "\n",
    "                # calculate total accuracy\n",
    "                if self.predict is not None:\n",
    "                    pred, acc = self.predict(output, targets, transform)\n",
    "                    temp_acc.append(acc)\n",
    "\n",
    "        self.loss_hist[dataset_name][self.current_epoch] = np.mean(temp_loss, 0)\n",
    "        if self.predict is not None:\n",
    "            self.acc_hist[dataset_name][self.current_epoch] = np.mean(temp_acc, 0)\n",
    "\n",
    "\n",
    "    def train(self, num_epochs, verbosity=1):\n",
    "\n",
    "        self.net.to(device)\n",
    "\n",
    "        # Validation\n",
    "        self.test(\"validation\")\n",
    "        if verbosity:\n",
    "            print(f\"Epoch {self.current_epoch}:\")\n",
    "            print(f\"Validation Loss = {self.loss_hist['validation'][self.current_epoch]}\")\n",
    "            if self.predict is not None:\n",
    "                print(f\"Validation Accuracy = {self.acc_hist['validation'][self.current_epoch]}\")\n",
    "            print(\"\\n-------------------------------\\n\")\n",
    "\n",
    "        for epoch in tqdm(range(num_epochs), desc=\"Epoch\"):\n",
    "            self.net.train()\n",
    "            # Minibatch training loop\n",
    "            for data, targets in tqdm(self.datasets[\"train\"], desc=\"Batches\", leave=False):\n",
    "                data = data.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                # forward pass\n",
    "                output = self.net(data)\n",
    "                pred, _ = self.predict(output, targets)\n",
    "\n",
    "                # compute loss\n",
    "                loss_val = self.loss_fn(pred, targets)\n",
    "\n",
    "                # Gradient calculation + weight update\n",
    "                self.optimizer.zero_grad()\n",
    "                loss_val.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # Store loss history for future plotting\n",
    "                if self.current_epoch in self.loss_hist[\"train\"]:\n",
    "                    self.loss_hist[\"train\"][self.current_epoch].append(loss_val.item())\n",
    "                else:\n",
    "                    self.loss_hist[\"train\"][self.current_epoch] = [loss_val.item()]\n",
    "\n",
    "            self.current_epoch += 1\n",
    "\n",
    "            # Validation\n",
    "            self.test(\"validation\")\n",
    "\n",
    "            if verbosity:\n",
    "                print(f\"Epoch {self.current_epoch}:\")\n",
    "                print(f\"Validation Loss = {self.loss_hist['validation'][self.current_epoch]}\")\n",
    "                if self.predict is not None:\n",
    "                    print(f\"Validation Accuracy = {self.acc_hist['validation'][self.current_epoch]}\")\n",
    "                print(\"\\n-------------------------------\\n\")\n",
    "\n",
    "    \n",
    "    def plot_loss(self, validation=True, logscale=True):\n",
    "\n",
    "        loss = [l for l_per_epoch in self.loss_hist[\"train\"].values() for l in l_per_epoch]\n",
    "        fig = plt.figure(facecolor=\"w\", figsize=(10, 6))\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        if logscale:\n",
    "            plt.yscale(\"log\")\n",
    "        plt.plot(loss, label=\"Training\")\n",
    "        if validation:\n",
    "            x = [i*len(self.datasets[\"train\"]) for i in self.loss_hist[\"validation\"]]\n",
    "            plt.plot(x, list(self.loss_hist[\"validation\"].values()), color='orange', marker='o', linestyle='dashed', label=\"Validation\")\n",
    "        \n",
    "        plt.legend(loc='upper right')\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    def ConfusionMatrix(self, *args, **kwargs):\n",
    "\n",
    "        cm = MulticlassConfusionMatrix(*args, **kwargs)\n",
    "\n",
    "        self.net.eval()\n",
    "        with torch.no_grad():\n",
    "            for data, targets in self.datasets[\"test\"]:\n",
    "                data = data.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                # forward pass\n",
    "                output = self.net(data)\n",
    "\n",
    "                # calculate total accuracy\n",
    "                pred, acc = self.predict(output, targets)\n",
    "                cm.update(pred, targets)\n",
    "        \n",
    "        return cm\n",
    "    \n",
    "\n",
    "    def plot_pred_vs_target(self, nbins, transform=None, logscale=False, *args, **kwargs):\n",
    "\n",
    "        self.net.eval()\n",
    "        all_targets = []\n",
    "        all_predictions = []\n",
    "        with torch.no_grad():\n",
    "            for data, targets in self.datasets[\"test\"]:\n",
    "                data = data.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                # forward pass\n",
    "                output = self.net(data)\n",
    "\n",
    "                # calculate total accuracy\n",
    "                pred, acc = self.predict(output, targets, transform)\n",
    "\n",
    "                if transform:\n",
    "                    targets = transform(targets)\n",
    "                all_targets.extend(targets.tolist())\n",
    "                all_predictions.extend(pred.tolist())\n",
    "\n",
    "        fig = plt.figure(facecolor=\"w\", figsize=(10, 6))\n",
    "        plt.xlabel(\"Targets\")\n",
    "        plt.ylabel(\"Prediction\")\n",
    "        if logscale:\n",
    "            plt.hist2d(all_targets, all_predictions, nbins, norm=SymLogNorm(*args, **kwargs), cmap='viridis')\n",
    "        else:\n",
    "            plt.hist2d(all_targets, all_predictions, nbins)\n",
    "\n",
    "        # Add a color bar\n",
    "        plt.colorbar(label='Counts')\n",
    "        plt.plot([0, 1e5], [0, 1e5], color='white', linewidth=1, linestyle='--')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, data_val = ds.build_dataset(\"../Data\", max_files=5, target=\"particle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "nw = 0\n",
    "train_load = DataLoader(data_train[:(len(data_train)//batch_size)*batch_size], batch_size=batch_size, shuffle=True, num_workers=nw)\n",
    "val_load   = DataLoader(data_val[:(len(data_val)//batch_size)*batch_size],     batch_size=batch_size, shuffle=True, num_workers=nw)\n",
    "test_load  = DataLoader(data_test[:(len(data_test)//batch_size)*batch_size],   batch_size=batch_size, shuffle=True, num_workers=nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter([d[1] for d in data_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss = nn.MSELoss()\n",
    "num_inputs = 100\n",
    "num_hidden = 200\n",
    "num_outputs = nClasses\n",
    "num_steps = timesteps\n",
    "net = Net_LIF_1L(num_inputs, num_hidden, num_outputs, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_accuracy(output, targets):\n",
    "    _, predicted = output.sum(dim=0).max(1)\n",
    "    total = targets.size(0)\n",
    "    correct = (predicted == targets).sum().item()\n",
    "\n",
    "    return predicted, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = SF.ce_count_loss(num_classes=nClasses)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "\n",
    "train_net = Trainer(net, loss_fn, optimizer, train_load, val_load, test_load, comp_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "train_net.train(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_net.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_net.test(\"test\", mc=True, num_classes=nClasses)\n",
    "print(train_net.loss_hist[\"test\"])\n",
    "train_net.cm[\"test\"].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, data_val = ds.build_dataset(\"../Data/PrimaryOnly2\",  primary_only=True, target=\"energy\",\n",
    "                                                   transform=lambda x: (x[0], ds.to_tensor_and_dtype(np.log10(x[1]), torch.float32)))\n",
    "\n",
    "batch_size = 50\n",
    "nw = 0\n",
    "train_load = DataLoader(data_train[:(len(data_train)//batch_size)*batch_size], batch_size=batch_size, shuffle=True, num_workers=nw)\n",
    "val_load   = DataLoader(data_val[:(len(data_val)//batch_size)*batch_size],     batch_size=batch_size, shuffle=True, num_workers=nw)\n",
    "test_load  = DataLoader(data_test[:(len(data_test)//batch_size)*batch_size],   batch_size=batch_size, shuffle=True, num_workers=nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energies = ((np.array([d[1] for d in data_train])))\n",
    "\n",
    "fig = plt.figure(facecolor=\"w\", figsize=(10, 6))\n",
    "plt.xlabel(\"log(Energy [MeV])\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.yscale(\"log\")\n",
    "plt.hist(energies)\n",
    "\n",
    "print(f\"Minimum energy: {min(energies)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_desc_regression = {\n",
    "    \"layers\" : [100, 200, 1],\n",
    "    \"timesteps\": 100,\n",
    "    \"output\": \"membrane\",\n",
    "    \"neuron_params\" : {\n",
    "                1: [snn.Leaky, \n",
    "                    {\"beta\" : 1.0,\n",
    "                    \"learn_beta\": True,\n",
    "                    \"threshold\" : 1.0,\n",
    "                    \"learn_threshold\": True,\n",
    "                    \"spike_grad\": surrogate.fast_sigmoid(),\n",
    "                    }],\n",
    "                2: [snn.Leaky, \n",
    "                    {\"beta\" : 1.0,\n",
    "                    \"learn_beta\": True,\n",
    "                    \"threshold\" : 1.0e20,\n",
    "                    \"learn_threshold\": False, \n",
    "                    \"spike_grad\": surrogate.fast_sigmoid(),\n",
    "                    }]\n",
    "                }\n",
    "    }\n",
    "\n",
    "def _spikegen(data):\n",
    "    spike_data = data.transpose(0,1) \n",
    "    spike_train = torch.where(spike_data > 300, torch.tensor(1), torch.tensor(0)).to(torch.float32) \n",
    "    return spike_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Membrane Potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_desc_regression_multi = {\n",
    "    \"layers\" : [400, 200, 1],\n",
    "    \"timesteps\": 100,\n",
    "    \"output\": \"membrane\",\n",
    "    \"neuron_params\" : {\n",
    "                1: [snn.Leaky, \n",
    "                    {\"beta\" : 1.0,\n",
    "                    \"learn_beta\": True,\n",
    "                    \"threshold\" : 1.0,\n",
    "                    \"learn_threshold\": True,\n",
    "                    \"spike_grad\": surrogate.fast_sigmoid(),\n",
    "                    }],\n",
    "                2: [snn.Leaky, \n",
    "                    {\"beta\" : 1.0,\n",
    "                    \"learn_beta\": True,\n",
    "                    \"threshold\" : 1.0e20,\n",
    "                    \"learn_threshold\": False, \n",
    "                    \"spike_grad\": surrogate.fast_sigmoid(),\n",
    "                    }]\n",
    "                }\n",
    "    }\n",
    "\n",
    "def spikegen_multi(data, multiplicity=4):\n",
    "    og_shape = data.shape\n",
    "    spike_data = torch.zeros(og_shape[1], og_shape[0], multiplicity*og_shape[2])\n",
    "    for i in range(multiplicity):\n",
    "        condition = data > np.power(10, i+2)\n",
    "        batch_idx, time_idx, sensor_idx = torch.nonzero(condition, as_tuple=True)\n",
    "        spike_data[time_idx, batch_idx, multiplicity*sensor_idx+i] = 1\n",
    "\n",
    "    return spike_data\n",
    "\n",
    "def predict_from_mem(output, targets, transform=None):\n",
    "    prediction = output[-1].squeeze(1)\n",
    "    if transform:\n",
    "        prediction, targets = transform(prediction), transform(targets)\n",
    "    accuracy = torch.abs((targets - prediction)/targets)\n",
    "    return prediction, torch.mean(accuracy)\n",
    "\n",
    "net = Spiking_Net(net_desc_regression_multi, lambda x: spikegen_multi(x,4))\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "train_net = Trainer(net, loss_fn, optimizer, train_load, val_load, test_load, predict_from_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "train_net.train(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_net.test(\"test\")\n",
    "print(f\"Test loss: {train_net.loss_hist['test'][num_epochs]}\")\n",
    "print(f\"Test relative error: {train_net.acc_hist['test'][num_epochs]*100}%\")\n",
    "train_net.plot_loss()\n",
    "train_net.plot_pred_vs_target(50, logscale=False, linthresh=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def antitransform(x):\n",
    "    return torch.pow(10, x)\n",
    "\n",
    "train_net.test(\"test\", transform=antitransform)\n",
    "print(f\"Test loss: {train_net.loss_hist['test'][num_epochs]}\")\n",
    "print(f\"Test relative error: {train_net.acc_hist['test'][num_epochs]*100}%\")\n",
    "train_net.plot_pred_vs_target(50, transform=antitransform, logscale=True, linthresh=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpikeFreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = 10\n",
    "net_desc_spikefreq = {\n",
    "    \"layers\" : [400, 200, population],\n",
    "    \"timesteps\": 100,\n",
    "    \"output\": \"membrane\",\n",
    "    \"neuron_params\" : {\n",
    "                1: [snn.Leaky, \n",
    "                    {\"beta\" : 1.0,\n",
    "                    \"learn_beta\": True,\n",
    "                    \"threshold\" : 1.0,\n",
    "                    \"learn_threshold\": True,\n",
    "                    \"spike_grad\": surrogate.fast_sigmoid(),\n",
    "                    }],\n",
    "                2: [snn.Leaky, \n",
    "                    {\"beta\" : 1.0,\n",
    "                    \"learn_beta\": True,\n",
    "                    \"threshold\" : 1.0,\n",
    "                    \"learn_threshold\": True, \n",
    "                    \"spike_grad\": surrogate.fast_sigmoid(),\n",
    "                    }]\n",
    "                }\n",
    "    }\n",
    "\n",
    "def predict_spikefreq(output, targets, transform=None):\n",
    "    prediction = output.sum((0,2))/(output.shape[0]*output.shape[2])\n",
    "    if transform:\n",
    "        prediction, targets = transform(prediction), transform(targets)\n",
    "    accuracy = torch.abs((targets - prediction)/targets)\n",
    "    return prediction, torch.mean(accuracy)\n",
    "\n",
    "net = Spiking_Net(net_desc_spikefreq, lambda x: spikegen_multi(x,4))\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "train_net = Trainer(net, loss_fn, optimizer, train_load, val_load, test_load, predict_spikefreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=10\n",
    "train_net.train(num_epochs)\n",
    "\n",
    "train_net.test(\"test\")\n",
    "print(f\"Test loss: {train_net.loss_hist['test'][10]}\")\n",
    "print(f\"Test relative error: {train_net.acc_hist['test'][10]*100}%\")\n",
    "train_net.plot_loss()\n",
    "train_net.plot_pred_vs_target(50, logscale=False, linthresh=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def antitransform(x):\n",
    "    return torch.pow(10, x)\n",
    "\n",
    "train_net.test(\"test\", transform=antitransform)\n",
    "print(f\"Test loss: {train_net.loss_hist['test'][num_epochs]}\")\n",
    "print(f\"Test relative error: {train_net.acc_hist['test'][num_epochs]*100}%\")\n",
    "train_net.plot_pred_vs_target(50, transform=antitransform, logscale=True, linthresh=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "all_x = []\n",
    "all_predictions = []\n",
    "with torch.no_grad():\n",
    "    for data, targets in test_load:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        output = net(data)\n",
    "        # calculate total accuracy\n",
    "        pred, acc = predict_spikefreq(output, targets, antitransform)\n",
    "        all_predictions.extend(pred.tolist())\n",
    "        all_x.extend(data.sum((1,2)).tolist())\n",
    "        \n",
    "fig = plt.figure(facecolor=\"w\", figsize=(10, 6))\n",
    "plt.xlabel(\"Photon Count\")\n",
    "plt.ylabel(\"Energy Prediction [MeV]\")\n",
    "plt.hist2d(all_x, all_predictions, 50, norm=SymLogNorm(linthresh=1), cmap='viridis')\n",
    "# Add a color bar\n",
    "plt.colorbar(label='Counts')\n",
    "plt.plot([0, 1e5], [0, 1e5], color='white', linewidth=1, linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, data_val = ds.build_dataset(\"../Data/PrimaryOnly2\", max_files=5, primary_only=True,\n",
    "                                                   target=[\"energy\", \"centroid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, data_val = ds.build_dataset(\"../Data/All\", max_files=100, primary_only=False,\n",
    "                                                   target=[\"energy\", \"centroid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model     = LinearRegression()\n",
    "lin_model_log = LinearRegression()\n",
    "\n",
    "x, y = zip(*[data_train[i] for i in range(len(data_train))])\n",
    "E = np.array([y[0] for y in y])\n",
    "centroid = np.array([y[1] for y in y])\n",
    "#primary = np.array([y[2] for y in y])\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "x = torch.stack(x).sum((1,2)).numpy()  # Convert features to NumPy\n",
    "\n",
    "# eliminate results with no photons\n",
    "mask = x > 0\n",
    "x = x[mask]\n",
    "E = E[mask]\n",
    "centroid = centroid[mask]\n",
    "#primary = primary[mask]\n",
    "\n",
    "# convert to double log scale\n",
    "x_log = np.log10(x)\n",
    "E_log = np.log10(E)\n",
    "\n",
    "lin_model.fit(x.reshape(-1,1), E)\n",
    "lin_model_log.fit(x_log.reshape(-1,1), E_log)\n",
    "print(\"Linear Model:\")\n",
    "print(\"Slope:\", lin_model.coef_[0])\n",
    "print(\"Intercept:\", lin_model.intercept_)\n",
    "print(\"\\n###################\\n\\nLinear Model Trained on logs:\")\n",
    "print(\"Slope:\", lin_model_log.coef_[0])\n",
    "print(\"Intercept:\", lin_model_log.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(facecolor=\"w\", figsize=(10, 6))\n",
    "plt.xlabel(\"log(Energy [MeV])\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.yscale(\"log\")\n",
    "plt.hist(E_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = zip(*[data_train[i] for i in range(len(data_test))])\n",
    "E_test = np.array([y[0] for y in y_test])\n",
    "centroid_test = np.array([y[1] for y in y_test])\n",
    "#primary_test = np.array([y[2] for y in y_test])\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "x_test = torch.stack(x_test).sum((1,2)).numpy()  # Convert features to NumPy\n",
    "\n",
    "# eliminate results with no photons\n",
    "mask = x_test > 0\n",
    "x_test = x_test[mask]\n",
    "E_test = E_test[mask]\n",
    "centroid_test = centroid_test[mask]\n",
    "#primary_test = primary_test[mask]\n",
    "\n",
    "x_test_log = np.log10(x_test)\n",
    "E_test_log = np.log10(E_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(16,6))\n",
    "\n",
    "x_plot = np.arange(0.1, np.max(x_test), 1e3)\n",
    "y_plot = lin_model.predict(x_plot.reshape(-1,1))\n",
    "x_plot_log = np.arange(0.1, np.max(x_test_log), 0.1)\n",
    "y_plot_log = lin_model_log.predict(x_plot_log.reshape(-1,1))\n",
    "\n",
    "ax[0].set_xlabel(\"log(Photon Count)\")\n",
    "ax[0].set_ylabel(\"log(Energy [MeV])\")\n",
    "hb0 = ax[0].hist2d(x_test_log, E_test_log, 50, norm=SymLogNorm(linthresh=1), cmap='viridis')\n",
    "ax[0].plot(np.log10(x_plot), np.log10(y_plot), color='white',\n",
    "           linewidth=2, linestyle='--', label=\"Linear Model Prediction\")\n",
    "ax[0].plot(x_plot_log, y_plot_log, color='#FF00FF',\n",
    "           linewidth=2, linestyle='--', label=\"Log Model Prediction\")\n",
    "cbar0 = fig.colorbar(hb0[3], ax=ax[0])\n",
    "cbar0.set_label('Data')  # Label for the first colorbar\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].set_xlabel(\"Photon Count\")\n",
    "ax[1].set_ylabel(\"Energy [MeV]\")\n",
    "hb1 = ax[1].hist2d(x_test, E_test, 50, norm=SymLogNorm(linthresh=1), cmap='viridis')\n",
    "ax[1].plot(x_plot, y_plot, color='white',\n",
    "           linewidth=1, linestyle='--', label=\"Linear Model Prediction\")\n",
    "ax[1].plot(np.pow(10,x_plot_log), np.pow(10,y_plot_log), color='#FF00FF',\n",
    "           linewidth=1, linestyle='--', label=\"Log Model Prediction\")\n",
    "cbar1 = fig.colorbar(hb1[3], ax=ax[1])\n",
    "cbar1.set_label('Data')  # Label for the first colorbar\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(16,6))\n",
    "\n",
    "condition = centroid_test[:,1] < 5\n",
    "\n",
    "ax[0].set_title(\"Y idx < 5\")\n",
    "ax[0].set_ylabel(\"Energy [MeV]\")\n",
    "hb0 = ax[0].hist2d(x_test[condition], E_test[condition], 50, norm=SymLogNorm(linthresh=1, vmax=1e4, vmin=0), cmap='viridis')\n",
    "ax[0].plot(x_plot, y_plot, color='white',\n",
    "           linewidth=1, linestyle='--', label=\"Linear Model Prediction\")\n",
    "ax[0].plot(np.pow(10,x_plot_log), np.pow(10,y_plot_log), color='#FF00FF',\n",
    "           linewidth=1, linestyle='--', label=\"Log Model Prediction\")\n",
    "ax[0].legend()\n",
    "\n",
    "\n",
    "condition = centroid_test[:,1] >= 5\n",
    "\n",
    "ax[1].set_title(\"Y idx >= 5\")\n",
    "hb1 = ax[1].hist2d(x_test[condition], E_test[condition], 50, norm=SymLogNorm(linthresh=1, vmax=1e4, vmin=0), cmap='viridis')\n",
    "ax[1].plot(x_plot, y_plot, color='white',\n",
    "           linewidth=1, linestyle='--', label=\"Linear Model Prediction\")\n",
    "ax[1].plot(np.pow(10,x_plot_log), np.pow(10,y_plot_log), color='#FF00FF',\n",
    "           linewidth=1, linestyle='--', label=\"Log Model Prediction\")\n",
    "ax[1].legend()\n",
    "\n",
    "cbar1 = fig.colorbar(hb1[3], ax=ax)\n",
    "cbar1.set_label('Data') \n",
    "fig.text(0.5, 0.04, 'Photon Count', ha='center')\n",
    "#plt.tight_layout(rect=[0.05, 0.05, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_pred = lin_model.predict(x_test.reshape(-1,1))\n",
    "acc = np.mean(abs((E_test-E_pred)/E_test)*100)\n",
    "print(f\"Mean relative error: {acc}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_pred_log = lin_model_log.predict(x_test_log.reshape(-1,1))\n",
    "acc = np.mean(abs((np.pow(10,E_test_log)-np.pow(10,E_pred_log))/np.pow(10,E_test_log))*100)\n",
    "print(f\"Mean relative error: {acc}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_up   = E_test_log > lin_model_log.predict(x_test_log.reshape(-1,1))\n",
    "mask_down = E_test_log < lin_model_log.predict(x_test_log.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=1, figsize=(8,6))\n",
    "ax.hist(centroid_test[mask_up, 1], bins=[-0.5+i for i in range(11)], \n",
    "        edgecolor='blue', facecolor='white', linewidth=1.5, label='E > prediction', alpha=0.8)\n",
    "ax.hist(centroid_test[mask_down, 1], bins=[-0.5+i for i in range(11)], \n",
    "        edgecolor='orange', facecolor='white', linewidth=1.5, label='E < prediction', alpha=0.8)\n",
    "ax.hist(centroid_test[:, 1], bins=[-0.5+i for i in range(11)], \n",
    "        edgecolor='grey', facecolor='lightgrey', alpha=0.2, linewidth=1.5, label='Global')\n",
    "ax.set_xticks([i for i in range(10)])\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Centroid Y cell\")\n",
    "ax.set_ylabel(\"Counts per cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy Centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, data_val = ds.build_dataset(\"../Data/PrimaryOnly2\",  primary_only=True, target=\"centroid\",\n",
    "                                                   transform=lambda x: (x[0], ds.to_tensor_and_dtype(x[1][0], torch.float32)))\n",
    "batch_size = 50\n",
    "nw = 0\n",
    "train_load = DataLoader(data_train[:(len(data_train)//batch_size)*batch_size], batch_size=batch_size, shuffle=True, num_workers=nw)\n",
    "val_load   = DataLoader(data_val[:(len(data_val)//batch_size)*batch_size],     batch_size=batch_size, shuffle=True, num_workers=nw)\n",
    "test_load  = DataLoader(data_test[:(len(data_test)//batch_size)*batch_size],   batch_size=batch_size, shuffle=True, num_workers=nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ((np.array([d[1] for d in data_train])))\n",
    "\n",
    "fig = plt.figure(facecolor=\"w\", figsize=(10, 6))\n",
    "plt.xlabel(\"X coordinate\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.yscale(\"log\")\n",
    "plt.hist(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = 10\n",
    "net_desc_spikefreq = {\n",
    "    \"layers\" : [400, 200, population],\n",
    "    \"timesteps\": 100,\n",
    "    \"output\": \"membrane\",\n",
    "    \"neuron_params\" : {\n",
    "                1: [snn.Leaky, \n",
    "                    {\"beta\" : 1.0,\n",
    "                    \"learn_beta\": True,\n",
    "                    \"threshold\" : 1.0,\n",
    "                    \"learn_threshold\": True,\n",
    "                    \"spike_grad\": surrogate.fast_sigmoid(),\n",
    "                    }],\n",
    "                2: [snn.Leaky, \n",
    "                    {\"beta\" : 1.0,\n",
    "                    \"learn_beta\": True,\n",
    "                    \"threshold\" : 1.0,\n",
    "                    \"learn_threshold\": True, \n",
    "                    \"spike_grad\": surrogate.fast_sigmoid(),\n",
    "                    }]\n",
    "                }\n",
    "    }\n",
    "\n",
    "def predict_spikefreq(output, targets, transform=None):\n",
    "    prediction = output.sum((0,2))/(output.shape[0]*output.shape[2])\n",
    "    if transform:\n",
    "        prediction, targets = transform(prediction), transform(targets)\n",
    "    accuracy = torch.abs((targets - prediction)/targets)\n",
    "    return prediction, torch.mean(accuracy)\n",
    "\n",
    "net_x = Spiking_Net(net_desc_spikefreq, lambda x: spikegen_multi(x,4))\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer_x = torch.optim.Adam(net_x.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "train_net_x = Trainer(net_x, loss_fn, optimizer_x, train_load, val_load, test_load, predict_spikefreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=10\n",
    "train_net_x.train(num_epochs)\n",
    "\n",
    "train_net_x.test(\"test\")\n",
    "print(f\"Test loss: {train_net_x.loss_hist['test'][10]}\")\n",
    "print(f\"Test relative error: {train_net_x.acc_hist['test'][10]*100}%\")\n",
    "train_net_x.plot_loss()\n",
    "train_net_x.plot_pred_vs_target(50, logscale=False, linthresh=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, data_val = ds.build_dataset(\"../Data/PrimaryOnly2\",  primary_only=True, target=\"centroid\",\n",
    "                                                   transform=lambda x: (x[0], ds.to_tensor_and_dtype(x[1][2], torch.float32)))\n",
    "batch_size = 50\n",
    "nw = 0\n",
    "train_load = DataLoader(data_train[:(len(data_train)//batch_size)*batch_size], batch_size=batch_size, shuffle=True, num_workers=nw)\n",
    "val_load   = DataLoader(data_val[:(len(data_val)//batch_size)*batch_size],     batch_size=batch_size, shuffle=True, num_workers=nw)\n",
    "test_load  = DataLoader(data_test[:(len(data_test)//batch_size)*batch_size],   batch_size=batch_size, shuffle=True, num_workers=nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = ((np.array([d[1] for d in data_val])))\n",
    "\n",
    "fig = plt.figure(facecolor=\"w\", figsize=(10, 6))\n",
    "plt.xlabel(\"Z coordinate\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.yscale(\"log\")\n",
    "plt.hist(Z, bins=[i for i in range(11)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_z = Spiking_Net(net_desc_spikefreq, lambda x: spikegen_multi(x,4))\n",
    "\n",
    "optimizer_z = torch.optim.Adam(net_z.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "train_net_z = Trainer(net_z, loss_fn, optimizer_z, train_load, val_load, test_load, predict_spikefreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=10\n",
    "train_net_z.train(num_epochs)\n",
    "\n",
    "train_net_z.test(\"test\")\n",
    "print(f\"Test loss: {train_net_z.loss_hist['test'][10]}\")\n",
    "print(f\"Test relative error: {train_net_z.acc_hist['test'][10]*100}%\")\n",
    "train_net_z.plot_loss()\n",
    "train_net_z.plot_pred_vs_target(50, logscale=False, linthresh=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, data_val = ds.build_dataset(\"../Data/PrimaryOnly2\",  primary_only=True, target=\"centroid\",\n",
    "                                                   transform=lambda x: (x[0], ds.to_tensor_and_dtype(x[1][1], torch.float32)))\n",
    "batch_size = 50\n",
    "nw = 0\n",
    "train_load = DataLoader(data_train[:(len(data_train)//batch_size)*batch_size], batch_size=batch_size, shuffle=True, num_workers=nw)\n",
    "val_load   = DataLoader(data_val[:(len(data_val)//batch_size)*batch_size],     batch_size=batch_size, shuffle=True, num_workers=nw)\n",
    "test_load  = DataLoader(data_test[:(len(data_test)//batch_size)*batch_size],   batch_size=batch_size, shuffle=True, num_workers=nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = ((np.array([d[1] for d in data_val])))\n",
    "\n",
    "fig = plt.figure(facecolor=\"w\", figsize=(10, 6))\n",
    "plt.xlabel(\"Y coordinate\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.yscale(\"log\")\n",
    "plt.hist(Y, bins=[i for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_y = Spiking_Net(net_desc_spikefreq, lambda x: spikegen_multi(x,4))\n",
    "\n",
    "optimizer_y = torch.optim.Adam(net_y.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "train_net_y = Trainer(net_y, loss_fn, optimizer_y, train_load, val_load, test_load, predict_spikefreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=10\n",
    "train_net_y.train(num_epochs)\n",
    "\n",
    "train_net_y.test(\"test\")\n",
    "print(f\"Test loss: {train_net_y.loss_hist['test'][10]}\")\n",
    "print(f\"Test relative error: {train_net_y.acc_hist['test'][10]*100}%\")\n",
    "train_net_y.plot_loss()\n",
    "train_net_y.plot_pred_vs_target(50, logscale=False, linthresh=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X, Y and Z together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, data_val = ds.build_dataset(\"../Data/PrimaryOnly2\",  primary_only=True, target=\"centroid\",\n",
    "                                                   transform=lambda x: (x[0], ds.to_tensor_and_dtype(x[1], torch.float32)))\n",
    "batch_size = 50\n",
    "nw = 0\n",
    "train_load = DataLoader(data_train[:(len(data_train)//batch_size)*batch_size], batch_size=batch_size, shuffle=True, num_workers=nw)\n",
    "val_load   = DataLoader(data_val[:(len(data_val)//batch_size)*batch_size],     batch_size=batch_size, shuffle=True, num_workers=nw)\n",
    "test_load  = DataLoader(data_test[:(len(data_test)//batch_size)*batch_size],   batch_size=batch_size, shuffle=True, num_workers=nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 3]) torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(size=(100,50,30))\n",
    "b = torch.rand(size=(50,3))\n",
    "p, a = predict_spikefreq_multitask(a, b, 10)\n",
    "print(p.shape, a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = 10\n",
    "net_centroid = {\n",
    "    \"layers\" : [400, 200, 3*population],\n",
    "    \"timesteps\": 100,\n",
    "    \"output\": \"membrane\",\n",
    "    \"neuron_params\" : {\n",
    "                1: [snn.Leaky, \n",
    "                    {\"beta\" : 1.0,\n",
    "                    \"learn_beta\": True,\n",
    "                    \"threshold\" : 1.0,\n",
    "                    \"learn_threshold\": True,\n",
    "                    \"spike_grad\": surrogate.fast_sigmoid(),\n",
    "                    }],\n",
    "                2: [snn.Leaky, \n",
    "                    {\"beta\" : 1.0,\n",
    "                    \"learn_beta\": True,\n",
    "                    \"threshold\" : 1.0,\n",
    "                    \"learn_threshold\": True, \n",
    "                    \"spike_grad\": surrogate.fast_sigmoid(),\n",
    "                    }]\n",
    "                }\n",
    "    }\n",
    "\n",
    "def predict_spikefreq_multitask(output, targets, transform=None):\n",
    "    population=10\n",
    "    output = output.reshape(output.shape[0], output.shape[1], population, -1)\n",
    "    prediction = output.sum((0,2))/(output.shape[0]*output.shape[2])\n",
    "    if transform:\n",
    "        prediction, targets = transform(prediction), transform(targets)\n",
    "    accuracy = torch.abs((targets - prediction)/targets)\n",
    "    return prediction, torch.mean(accuracy, 0)\n",
    "\n",
    "net_c = Spiking_Net(net_centroid, lambda x: spikegen_multi(x,4))\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer_c = torch.optim.Adam(net_c.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "train_net_c = Trainer(net_c, loss_fn, optimizer_c, train_load, val_load, test_load, predict_spikefreq_multitask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "Validation Loss = 38.27633317987969\n",
      "Validation Accuracy = [1.2091588 1.3594581 1.0016282]\n",
      "\n",
      "-------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|██        | 1/5 [00:30<02:03, 30.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Validation Loss = 6.732227112384552\n",
      "Validation Accuracy = [1.2404289  1.601664   0.25510627]\n",
      "\n",
      "-------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|████      | 2/5 [01:00<01:31, 30.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\n",
      "Validation Loss = 3.3797996247068363\n",
      "Validation Accuracy = [0.79108924 0.9402199  0.16917829]\n",
      "\n",
      "-------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|██████    | 3/5 [01:31<01:01, 30.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:\n",
      "Validation Loss = 1.86167729408183\n",
      "Validation Accuracy = [0.7274307  0.7752199  0.10956288]\n",
      "\n",
      "-------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|████████  | 4/5 [02:02<00:30, 30.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:\n",
      "Validation Loss = 1.2906953984118523\n",
      "Validation Accuracy = [0.7495827  0.69653237 0.08865261]\n",
      "\n",
      "-------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 5/5 [02:31<00:00, 30.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:\n",
      "Validation Loss = 1.118674190754586\n",
      "Validation Accuracy = [0.70529044 0.6151617  0.0778795 ]\n",
      "\n",
      "-------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_net_c.train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: np.float32(1.4504564),\n",
       " 1: np.float32(0.9516595),\n",
       " 2: np.float32(0.57651806),\n",
       " 3: np.float32(0.5132448),\n",
       " 4: np.float32(0.48001555),\n",
       " 5: np.float32(0.40587848)}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_net_c.acc_hist[\"validation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy Dispersion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, data_val = ds.build_dataset(\"../Data/PrimaryOnly2\", max_files=5, target=\"sigmaR\",\n",
    "                                                   transform= lambda x: (x[0], ds.to_tensor_and_dtype(x[1], torch.float32)))\n",
    "\n",
    "batch_size = 50\n",
    "nw = 0\n",
    "train_load = DataLoader(data_train[:(len(data_train)//batch_size)*batch_size], batch_size=batch_size, shuffle=True, num_workers=nw)\n",
    "val_load   = DataLoader(data_val[:(len(data_val)//batch_size)*batch_size],     batch_size=batch_size, shuffle=True, num_workers=nw)\n",
    "test_load  = DataLoader(data_test[:(len(data_test)//batch_size)*batch_size],   batch_size=batch_size, shuffle=True, num_workers=nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmaR = ((np.array([d[1] for d in data_train])))\n",
    "\n",
    "fig = plt.figure(facecolor=\"w\", figsize=(10, 6))\n",
    "plt.xlabel(r'$\\sigma_{R}$', fontsize=14)\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.yscale(\"log\")\n",
    "plt.hist(sigmaR)\n",
    "\n",
    "#print(f\"Minimum energy: {min(energies)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_sR = Spiking_Net(net_desc_spikefreq, lambda x: spikegen_multi(x,4))\n",
    "\n",
    "optimizer_sR = torch.optim.Adam(net_sR.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "train_net_sR = Trainer(net_sR, loss_fn, optimizer_sR, train_load, val_load, test_load, predict_spikefreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=10\n",
    "train_net_sR.train(num_epochs)\n",
    "\n",
    "train_net_sR.test(\"test\")\n",
    "print(f\"Test loss: {train_net_sR.loss_hist['test'][num_epochs]}\")\n",
    "print(f\"Test relative error: {train_net_sR.acc_hist['test'][num_epochs]*100}%\")\n",
    "train_net_sR.plot_loss()\n",
    "train_net_sR.plot_pred_vs_target(50, logscale=False, linthresh=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, data_val = ds.build_dataset(\"../Data/PrimaryOnly2\", max_files=5, target=\"sigmaZ\",\n",
    "                                                   transform= lambda x: (x[0], ds.to_tensor_and_dtype(x[1], torch.float32)))\n",
    "\n",
    "batch_size = 50\n",
    "nw = 0\n",
    "train_load = DataLoader(data_train[:(len(data_train)//batch_size)*batch_size], batch_size=batch_size, shuffle=True, num_workers=nw)\n",
    "val_load   = DataLoader(data_val[:(len(data_val)//batch_size)*batch_size],     batch_size=batch_size, shuffle=True, num_workers=nw)\n",
    "test_load  = DataLoader(data_test[:(len(data_test)//batch_size)*batch_size],   batch_size=batch_size, shuffle=True, num_workers=nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmaZ = ((np.array([d[1] for d in data_train])))\n",
    "\n",
    "fig = plt.figure(facecolor=\"w\", figsize=(10, 6))\n",
    "plt.xlabel(r'$\\sigma_{Z}$', fontsize=14)\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.yscale(\"log\")\n",
    "plt.hist(sigmaZ)\n",
    "\n",
    "#print(f\"Minimum energy: {min(energies)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_sZ = Spiking_Net(net_desc_spikefreq, lambda x: spikegen_multi(x,4))\n",
    "\n",
    "optimizer_sZ = torch.optim.Adam(net_sZ.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "train_net_sZ = Trainer(net_sZ, loss_fn, optimizer_sZ, train_load, val_load, test_load, predict_spikefreq)\n",
    "\n",
    "num_epochs=10\n",
    "train_net_sZ.train(num_epochs)\n",
    "\n",
    "train_net_sZ.test(\"test\")\n",
    "print(f\"Test loss: {train_net_sZ.loss_hist['test'][num_epochs]}\")\n",
    "print(f\"Test relative error: {train_net_sZ.acc_hist['test'][num_epochs]*100}%\")\n",
    "train_net_sZ.plot_loss()\n",
    "train_net_sZ.plot_pred_vs_target(50, logscale=False, linthresh=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy Regression + Y Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, data_val = ds.build_dataset(\"../Data/PrimaryOnly2\",  primary_only=True, target=\"energy\",\n",
    "                                                   transform=lambda x: (x[0], ds.to_tensor_and_dtype(np.log10(x[1]), torch.float32)))\n",
    "\n",
    "batch_size = 50\n",
    "nw = 0\n",
    "train_load = DataLoader(data_train[:(len(data_train)//batch_size)*batch_size], batch_size=batch_size, shuffle=True, num_workers=nw)\n",
    "val_load   = DataLoader(data_val[:(len(data_val)//batch_size)*batch_size],     batch_size=batch_size, shuffle=True, num_workers=nw)\n",
    "test_load  = DataLoader(data_test[:(len(data_test)//batch_size)*batch_size],   batch_size=batch_size, shuffle=True, num_workers=nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spikegen_multi(data, multiplicity=4):\n",
    "    og_shape = data.shape\n",
    "    spike_data = torch.zeros(og_shape[1], og_shape[0], multiplicity*og_shape[2])\n",
    "    for i in range(multiplicity):\n",
    "        condition = data > np.power(10, i+2)\n",
    "        batch_idx, time_idx, sensor_idx = torch.nonzero(condition, as_tuple=True)\n",
    "        spike_data[time_idx, batch_idx, multiplicity*sensor_idx+i] = 1\n",
    "\n",
    "    return spike_data\n",
    "\n",
    "def spikegen_with_position(data, *args, **kwargs):\n",
    "    spk1 = spikegen_multi(data, *args, **kwargs)\n",
    "    spk2 = net_y(data)\n",
    "\n",
    "    output = torch.cat((spk1, spk2), dim=2)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "population = 10\n",
    "net_desc_spikefreq_with_pos = {\n",
    "    \"layers\" : [410, 200, population],\n",
    "    \"timesteps\": 100,\n",
    "    \"output\": \"membrane\",\n",
    "    \"neuron_params\" : {\n",
    "                1: [snn.Leaky, \n",
    "                    {\"beta\" : 1.0,\n",
    "                    \"learn_beta\": True,\n",
    "                    \"threshold\" : 1.0,\n",
    "                    \"learn_threshold\": True,\n",
    "                    \"spike_grad\": surrogate.fast_sigmoid(),\n",
    "                    }],\n",
    "                2: [snn.Leaky, \n",
    "                    {\"beta\" : 1.0,\n",
    "                    \"learn_beta\": True,\n",
    "                    \"threshold\" : 1.0,\n",
    "                    \"learn_threshold\": True, \n",
    "                    \"spike_grad\": surrogate.fast_sigmoid(),\n",
    "                    }]\n",
    "                }\n",
    "    }\n",
    "\n",
    "net_E = Spiking_Net(net_desc_spikefreq_with_pos, lambda x: spikegen_with_position(x,4))\n",
    "\n",
    "optimizer_E = torch.optim.Adam(net_E.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "train_net_E = Trainer(net_E, loss_fn, optimizer_E, train_load, val_load, test_load, predict_spikefreq)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=10\n",
    "train_net_E.train(num_epochs)\n",
    "\n",
    "train_net_E.test(\"test\")\n",
    "print(f\"Test loss: {train_net_E.loss_hist['test'][10]}\")\n",
    "print(f\"Test relative error: {train_net_E.acc_hist['test'][10]*100}%\")\n",
    "train_net_E.plot_loss()\n",
    "train_net_E.plot_pred_vs_target(50, logscale=False, linthresh=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def antitransform(x):\n",
    "    return torch.pow(10, x)\n",
    "\n",
    "train_net_E.test(\"test\", transform=antitransform)\n",
    "print(f\"Test loss: {train_net_E.loss_hist['test'][num_epochs]}\")\n",
    "print(f\"Test relative error: {train_net_E.acc_hist['test'][num_epochs]*100}%\")\n",
    "train_net_E.plot_pred_vs_target(50, transform=antitransform, logscale=True, linthresh=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_E.eval()\n",
    "all_x = []\n",
    "all_predictions = []\n",
    "with torch.no_grad():\n",
    "    for data, targets in test_load:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        output = net_E(data)\n",
    "        # calculate total accuracy\n",
    "        pred, acc = predict_spikefreq(output, targets, antitransform)\n",
    "        all_predictions.extend(pred.tolist())\n",
    "        all_x.extend(data.sum((1,2)).tolist())\n",
    "        \n",
    "fig = plt.figure(facecolor=\"w\", figsize=(10, 6))\n",
    "plt.xlabel(\"Photon Count\")\n",
    "plt.ylabel(\"Energy Prediction [MeV]\")\n",
    "plt.hist2d(all_x, all_predictions, 50, norm=SymLogNorm(linthresh=1), cmap='viridis')\n",
    "# Add a color bar\n",
    "plt.colorbar(label='Counts')\n",
    "plt.plot([0, 1e5], [0, 1e5], color='white', linewidth=1, linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N of interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, data_val = ds.build_dataset(\"../Data/PrimaryOnly\", max_files=5, target=\"N_int\",\n",
    "                                                   transform= lambda x: (x[0], ds.to_tensor_and_dtype(x[1], torch.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "nw = 0\n",
    "train_load = DataLoader(data_train[:(len(data_train)//batch_size)*batch_size], batch_size=batch_size, shuffle=True, num_workers=nw)\n",
    "val_load   = DataLoader(data_val[:(len(data_val)//batch_size)*batch_size],     batch_size=batch_size, shuffle=True, num_workers=nw)\n",
    "test_load  = DataLoader(data_test[:(len(data_test)//batch_size)*batch_size],   batch_size=batch_size, shuffle=True, num_workers=nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nint = ((np.array([d[1] for d in data_train])))\n",
    "\n",
    "fig = plt.figure(facecolor=\"w\", figsize=(10, 6))\n",
    "plt.xlabel(r'$\\sigma_{E}$', fontsize=14)\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.yscale(\"log\")\n",
    "plt.hist(Nint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(Nint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 15\n",
    "print(Nint[idx])\n",
    "visualize.plot_views(idx, data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_LIF_1L(nn.Module):\n",
    "    \"\"\"FCN with 1 hidden layer and LIF neurons.\"\"\"\n",
    "\n",
    "    def __init__(self, input_feat, hidden, out_feat, timesteps,\n",
    "                 learnable=True, beta=0.8, threshold=1.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_feat = input_feat          # number of input neurons \n",
    "        self.hidden = hidden                  # number of hidden neurons\n",
    "        self.out_feat = out_feat              # number of output neurons\n",
    "        \n",
    "        self.timesteps = timesteps            # number of time steps to simulate the network\n",
    "\n",
    "        spike_grad = surrogate.fast_sigmoid() # surrogate gradient function\n",
    "        \n",
    "        self.fc_in = nn.Linear(in_features=self.input_feat, out_features=self.hidden)\n",
    "        self.lif_in = snn.Leaky(beta=beta, spike_grad=spike_grad, threshold=threshold,\n",
    "                                learn_beta=learnable, learn_threshold=learnable)\n",
    "        \n",
    "        self.fc_out = nn.Linear(in_features=self.hidden, out_features=self.out_feat)\n",
    "        self.lif_out = snn.Leaky(beta=beta, spike_grad=spike_grad, threshold=threshold,\n",
    "                                 learn_beta=learnable, learn_threshold=learnable)\n",
    "        \n",
    "    def _spikegen(self, data):\n",
    "        spike_data = data.transpose(0,1) \n",
    "        spike_train = torch.where(spike_data > 300, torch.tensor(1), torch.tensor(0)).to(torch.float32) \n",
    "        return spike_train\n",
    "    \n",
    "    def forward(self, data):\n",
    "        \"\"\"Forward pass for several time steps.\"\"\"\n",
    "\n",
    "        x = self._spikegen(data)\n",
    "\n",
    "        # Initalize membrane potential\n",
    "        mem1 = self.lif_in.init_leaky()\n",
    "        mem2 = self.lif_out.init_leaky()\n",
    "\n",
    "        # Record the final layer\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "\n",
    "        # Loop over \n",
    "        for step in range(self.timesteps):\n",
    "                \n",
    "            cur1 = self.fc_in(x[step])\n",
    "            spk1, mem1 = self.lif_in(cur1, mem1)\n",
    "            cur2 = self.fc_out(spk1)\n",
    "            spk2, mem2 = self.lif_out(cur2, mem2)\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "\n",
    "        return torch.stack(spk2_rec, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_Reg(nn.Module):\n",
    "    \"\"\"FCN with 1 hidden layer and LIF neurons.\"\"\"\n",
    "\n",
    "    def __init__(self, input_feat, hidden, timesteps,\n",
    "                 learnable=True, beta=0.8, threshold=1.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_feat = input_feat          # number of input neurons \n",
    "        self.hidden = hidden                  # number of hidden neurons\n",
    "        self.out_feat = 1                     # number of output neurons\n",
    "        \n",
    "        self.timesteps = timesteps            # number of time steps to simulate the network\n",
    "\n",
    "        spike_grad = surrogate.fast_sigmoid() # surrogate gradient function\n",
    "        \n",
    "        self.fc_in = nn.Linear(in_features=self.input_feat, out_features=self.hidden)\n",
    "        self.lif_in = snn.Leaky(beta=beta, spike_grad=spike_grad, threshold=threshold,\n",
    "                                learn_beta=learnable, learn_threshold=learnable)\n",
    "        \n",
    "        self.fc_out = nn.Linear(in_features=self.hidden, out_features=self.out_feat)\n",
    "        self.lif_out = snn.Leaky(beta=beta, spike_grad=spike_grad, threshold=1e20,\n",
    "                                 learn_beta=learnable)\n",
    "        \n",
    "    def _spikegen(self, data):\n",
    "        spike_data = data.transpose(0,1) \n",
    "        spike_train = torch.where(spike_data > 300, torch.tensor(1), torch.tensor(0)).to(torch.float32) \n",
    "        return spike_train\n",
    "    \n",
    "    def forward(self, data):\n",
    "        \"\"\"Forward pass for several time steps.\"\"\"\n",
    "\n",
    "        x = self._spikegen(data)\n",
    "\n",
    "        # Initalize membrane potential\n",
    "        mem1 = self.lif_in.init_leaky()\n",
    "        mem2 = self.lif_out.init_leaky()\n",
    "\n",
    "        # Record the final layer\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "\n",
    "        # Loop over \n",
    "        for step in range(self.timesteps):\n",
    "                \n",
    "            cur1 = self.fc_in(x[step])\n",
    "            spk1, mem1 = self.lif_in(cur1, mem1)\n",
    "            cur2 = self.fc_out(spk1)\n",
    "            spk2, mem2 = self.lif_out(cur2, mem2)\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "\n",
    "        return  mem2_rec[-1].squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss = nn.MSELoss()\n",
    "num_inputs = 100\n",
    "num_hidden = 200\n",
    "num_steps = timesteps\n",
    "net2 = Net_Reg(num_inputs, num_hidden, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_set(net, data_loader, loss_fn, comp_mc=False):\n",
    "    mcm = MulticlassConfusionMatrix(num_classes=nClasses, normalize='none')\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        temp_loss = []\n",
    "        for data, targets in data_loader:\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # create spike train\n",
    "            spike_in = custom_spikegen(data)\n",
    "            spike_in = spike_in.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            output = net(spike_in)\n",
    "\n",
    "            # compute loss\n",
    "            loss = loss_fn(output, targets)\n",
    "            temp_loss.append(loss.item())\n",
    "\n",
    "            # calculate total accuracy\n",
    "            tot, corr, pred = comp_accuracy(output, targets)\n",
    "            total += tot\n",
    "            correct += corr\n",
    "            if comp_mc:\n",
    "                mcm.update(pred, targets)\n",
    "\n",
    "        mean_loss = np.mean(temp_loss)\n",
    "        acc = correct/total\n",
    "\n",
    "        return mean_loss, acc, mcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(net, train_loader, val_loader, num_epochs, loss_fn, optimizer, batch_size=100):\n",
    "\n",
    "    net.to(device)\n",
    "    \n",
    "    loss_hist = []\n",
    "    loss_val_hist = []\n",
    "    acc_val_hist = []\n",
    "\n",
    "    iter_counter = 0\n",
    "\n",
    "    # Validation\n",
    "    mean_loss_val, acc_val, _ = accuracy_set(net, val_loader, loss_fn, batch_size)\n",
    "\n",
    "    loss_val_hist.append(mean_loss_val)\n",
    "    acc_val_hist.append(acc_val)\n",
    "    print(f\"Validation Set Loss: {mean_loss_val}\")\n",
    "    print(f\"Validation Set Accuracy: {100 *acc_val:.2f}%\")\n",
    "    print(\"\\n--------------------------------------------------\\n\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        batch_counter = 0\n",
    "        # Minibatch training loop\n",
    "        for data, targets in train_loader:\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # create spike train\n",
    "            spike_in = custom_spikegen(data)\n",
    "            spike_in = spike_in.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            output = net(spike_in)\n",
    "\n",
    "            # compute loss\n",
    "            loss_val = loss_fn(output, targets)\n",
    "        \n",
    "            # Gradient calculation + weight update\n",
    "            optimizer.zero_grad()\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Store loss history for future plotting\n",
    "            loss_hist.append(loss_val.item())\n",
    "        \n",
    "            if iter_counter % 50 == 0:\n",
    "                print(\"Epoch:\", epoch+1)\n",
    "                print(\"Batch:\", batch_counter)\n",
    "                print(\"Iteration:\", iter_counter)\n",
    "                print(\"Loss:\", loss_val.item(),\"\\n\")\n",
    "        \n",
    "            batch_counter += 1\n",
    "            iter_counter += 1\n",
    "\n",
    "        # Validation\n",
    "        mean_loss_val, acc_val, _ = accuracy_set(net, val_loader, loss_fn, batch_size)\n",
    "\n",
    "        loss_val_hist.append(mean_loss_val)\n",
    "        acc_val_hist.append(acc_val)\n",
    "        print(f\"Validation Set Loss: {mean_loss_val}\")\n",
    "        print(f\"Validation Set Accuracy: {100 *acc_val:.2f}%\")\n",
    "        print(\"\\n--------------------------------------------------\\n\")\n",
    "\n",
    "    return loss_hist, loss_val_hist, acc_val_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net_LIF_1L(num_inputs, num_hidden, num_outputs, num_steps).to(device)\n",
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "iter_counter = 0\n",
    "\n",
    "#loss = nn.MSELoss()\n",
    "num_inputs = 100\n",
    "num_hidden = 200\n",
    "num_outputs = nClasses\n",
    "num_steps = timesteps\n",
    "net = Net_LIF_1L(num_inputs, num_hidden, num_outputs, num_steps).to(device)\n",
    "loss_fn = SF.ce_count_loss(num_classes=nClasses)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "\n",
    "res = train_net(net, train_load, val_load, num_epochs, loss_fn, optimizer, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss\n",
    "fig = plt.figure(facecolor=\"w\", figsize=(20, 6))\n",
    "plt.plot(res[0])\n",
    "plt.title(\"Train loss\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "#plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss\n",
    "fig = plt.figure(facecolor=\"w\", figsize=(20, 6))\n",
    "plt.plot(res[1])\n",
    "plt.title(\"Validation loss\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "#plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = accuracy_set(net, test_load, loss_fn, True)\n",
    "test[2].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for data, targets in train_load:\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # create spike train\n",
    "            spike_in = custom_spikegen(data)\n",
    "            spike_in = spike_in.to(device)\n",
    "\n",
    "            print(spike_in.shape)\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snn-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
